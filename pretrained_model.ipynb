{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56be88473aeaaa14008c9080</td>\n",
       "      <td>In her music, what are some recurring elements...</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "      <td>love, relationships, and monogamy</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>56be892d3aeaaa14008c908c</td>\n",
       "      <td>Where did Beyonce get her name from?</td>\n",
       "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
       "      <td>her mother's maiden name</td>\n",
       "      <td>204.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>56bf74d53aeaaa14008c965a</td>\n",
       "      <td>Beyonce's mother worked in what industry?</td>\n",
       "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
       "      <td>hairdresser and salon owner</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>56bf76ef3aeaaa14008c9664</td>\n",
       "      <td>Which of her teachers discovered Beyonce's mus...</td>\n",
       "      <td>Beyoncé attended St. Mary's Elementary School ...</td>\n",
       "      <td>dance instructor Darlette Johnson</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>56e1039be3433e1400422aa6</td>\n",
       "      <td>Prior to 1917, what church was in a similar si...</td>\n",
       "      <td>The law of the Eastern Catholic Churches in fu...</td>\n",
       "      <td>the Latin or Western Church</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>56e1039be3433e1400422aa7</td>\n",
       "      <td>What was different about the Eastern Churches ...</td>\n",
       "      <td>The law of the Eastern Catholic Churches in fu...</td>\n",
       "      <td>more diversity in legislation</td>\n",
       "      <td>149.0</td>\n",
       "      <td>2824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>56e104b7e3433e1400422acb</td>\n",
       "      <td>What are the constituents of the Pēdálion?</td>\n",
       "      <td>The Greek-speaking Orthodox have collected can...</td>\n",
       "      <td>canons and commentaries upon them</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>56e10f57cd28a01900c674ff</td>\n",
       "      <td>In what institution do church courts still hav...</td>\n",
       "      <td>In the Church of England, the ecclesiastical c...</td>\n",
       "      <td>the Church of England</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "      <td>56e1156fe3433e1400422bb0</td>\n",
       "      <td>What is an example of a member of the Anglican...</td>\n",
       "      <td>Other churches in the Anglican Communion aroun...</td>\n",
       "      <td>the Anglican Church of Canada</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                     index  \\\n",
       "0              0  56be85543aeaaa14008c9063   \n",
       "1              1  56be88473aeaaa14008c9080   \n",
       "2              2  56be892d3aeaaa14008c908c   \n",
       "3              3  56bf74d53aeaaa14008c965a   \n",
       "4              4  56bf76ef3aeaaa14008c9664   \n",
       "...          ...                       ...   \n",
       "1995        1995  56e1039be3433e1400422aa6   \n",
       "1996        1996  56e1039be3433e1400422aa7   \n",
       "1997        1997  56e104b7e3433e1400422acb   \n",
       "1998        1998  56e10f57cd28a01900c674ff   \n",
       "1999        1999  56e1156fe3433e1400422bb0   \n",
       "\n",
       "                                               question  \\\n",
       "0              When did Beyonce start becoming popular?   \n",
       "1     In her music, what are some recurring elements...   \n",
       "2                  Where did Beyonce get her name from?   \n",
       "3             Beyonce's mother worked in what industry?   \n",
       "4     Which of her teachers discovered Beyonce's mus...   \n",
       "...                                                 ...   \n",
       "1995  Prior to 1917, what church was in a similar si...   \n",
       "1996  What was different about the Eastern Churches ...   \n",
       "1997         What are the constituents of the Pēdálion?   \n",
       "1998  In what institution do church courts still hav...   \n",
       "1999  What is an example of a member of the Anglican...   \n",
       "\n",
       "                                                context  \\\n",
       "0     Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1     A self-described \"modern-day feminist\", Beyonc...   \n",
       "2     Beyoncé Giselle Knowles was born in Houston, T...   \n",
       "3     Beyoncé Giselle Knowles was born in Houston, T...   \n",
       "4     Beyoncé attended St. Mary's Elementary School ...   \n",
       "...                                                 ...   \n",
       "1995  The law of the Eastern Catholic Churches in fu...   \n",
       "1996  The law of the Eastern Catholic Churches in fu...   \n",
       "1997  The Greek-speaking Orthodox have collected can...   \n",
       "1998  In the Church of England, the ecclesiastical c...   \n",
       "1999  Other churches in the Anglican Communion aroun...   \n",
       "\n",
       "                                   text  answer_start  c_id  \n",
       "0                     in the late 1990s         269.0     0  \n",
       "1     love, relationships, and monogamy         104.0     2  \n",
       "2              her mother's maiden name         204.0     3  \n",
       "3           hairdresser and salon owner         101.0     3  \n",
       "4     dance instructor Darlette Johnson         148.0     4  \n",
       "...                                 ...           ...   ...  \n",
       "1995        the Latin or Western Church         103.0  2824  \n",
       "1996      more diversity in legislation         149.0  2824  \n",
       "1997  canons and commentaries upon them          43.0  2826  \n",
       "1998              the Church of England           3.0  2827  \n",
       "1999      the Anglican Church of Canada         112.0  2828  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(\"train2000.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "ques=train.question   # all questions\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_ques = [word_tokenize(i) for i in ques] #word tokenization of all questions\n",
    "print(len(tokenized_ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238\n"
     ]
    }
   ],
   "source": [
    "paragra= train.context.unique()  # all unique paragraphs\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_para = [word_tokenize(i) for i in paragra] #word tokenization of all paragraphs\n",
    "print(len(tokenized_para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "paraorigi=train.context # all paragraphs\n",
    "tokenized_paraoriginal = [word_tokenize(i) for i in paraorigi]\n",
    "print(len(tokenized_paraoriginal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make sentence vectors from word vectors\n",
    "import numpy as np\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec) / numw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=[]  # list of all unique paragraph vectors\n",
    "\n",
    "for sentence in tokenized_para:\n",
    "    paragraph.append(sent_vectorizer(sentence, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=[]  # list of all question vectors\n",
    "for sentence in tokenized_ques:\n",
    "    question.append(sent_vectorizer(sentence, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraoriginal=[] # list of all paragraph vectors\n",
    "for sentence in tokenized_paraoriginal:\n",
    "    paraoriginal.append(sent_vectorizer(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosineValue(v1,v2):                  #compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [33:14<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "56.85\n",
      "64.55\n"
     ]
    }
   ],
   "source": [
    "# function  to find the accuracy of getting the correct paragraph out of the 3 and top 5 matched paragraphs \n",
    "count5=0   #  for top 5\n",
    "count3=0   #  for top 3\n",
    "for j in tqdm(range(len(question))):    #iterate over questions  # no. of questions=2000\n",
    "    ques1=[]\n",
    "    for i in range(len(paragraph)):     # iterate over paragraphs # no. of paragraph=1238\n",
    "         ques1.append(cosineValue (question[j], paragraph[i]))  # storing cosine value for a particular question with each paragraph\n",
    "    res = sorted(range(len(ques1)), key = lambda sub: ques1[sub])[-5:] #to store paragraph indices for top 5 cosine values\n",
    "    arr0=paragraph[res[0]]  # corresponding 5 paragraph vectors\n",
    "    arr1=paragraph[res[1]]\n",
    "    arr2=paragraph[res[2]]\n",
    "    arr3=paragraph[res[3]]\n",
    "    arr4=paragraph[res[4]]\n",
    "    \n",
    "    para=paraoriginal[j]  # original paragraph vector (paragraph vector corresponding to the question)\n",
    "\n",
    "    check0=para==arr0 #check whether the original vector is equal to one of the 5 vectors\n",
    "    check1=para==arr1\n",
    "    check2=para==arr2\n",
    "    check3=para==arr3\n",
    "    check4=para==arr4\n",
    "    if(check2.all() or check3.all() or check4.all() ): # for top 3\n",
    "        count3+=1\n",
    "    if(check0.all() or check1.all() or check2.all() or check3.all() or check4.all() ): # for top 5\n",
    "        count5+=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "print((count3/len(question))*100)\n",
    "print((count5/len(question))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anujkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# to make new vectors of questions by removing stopwords \n",
    "for i in range(len(ques)):\n",
    "    ques[i]=ques[i].lower()         # convert to lower case\n",
    "tokenized_ques = [word_tokenize(i) for i in ques]\n",
    "ques_new=[]\n",
    "for i in range(len(ques)):\n",
    "    filtered_sentence = [w for w in tokenized_ques[i] if not w in stop_words]  # storing words which are not stopwords\n",
    "\n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in tokenized_ques[i]: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)   \n",
    "    ques_new.append(filtered_sentence)\n",
    "question_new=[]                         #  list of all new question vectors\n",
    "for sentence in ques_new:\n",
    "    question_new.append(sent_vectorizer(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for all unique paragraph - removing stopwords\n",
    "for i in range(len(paragra)):\n",
    "    paragra[i]=paragra[i].lower()\n",
    "tokenized_para = [word_tokenize(i) for i in paragra]\n",
    "paragra_new=[]\n",
    "for i in range(len(paragra)):\n",
    "    filtered_sentence = [w for w in tokenized_para[i] if not w in stop_words] \n",
    "\n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in tokenized_para[i]: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)   \n",
    "    paragra_new.append(filtered_sentence)\n",
    "paragraph_new=[]                            #  list of all new unique paragraph vectors\n",
    "for sentence in paragra_new:\n",
    "    paragraph_new.append(sent_vectorizer(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anujkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#same for all paragraphs - removing stopwords\n",
    "for i in range(len(paraorigi)):\n",
    "    paraorigi[i]=paraorigi[i].lower()\n",
    "tokenized_paraoriginal = [word_tokenize(i) for i in paraorigi]\n",
    "paragraorigi_new=[]\n",
    "for i in range(len(paraoriginal)):\n",
    "    filtered_sentence = [w for w in tokenized_paraoriginal[i] if not w in stop_words] \n",
    "\n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in tokenized_paraoriginal[i]: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)   \n",
    "    paragraorigi_new.append(filtered_sentence)\n",
    "paraoriginal_new=[]                               #  list of all new paragraph vectors\n",
    "for sentence in paragraorigi_new:\n",
    "    paraoriginal_new.append(sent_vectorizer(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [17:32<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "61.25000000000001\n",
      "68.8\n"
     ]
    }
   ],
   "source": [
    "# same function  to find the accuracy but with questions and paragraphs after removing stopwords\n",
    "count3=0\n",
    "count5=0\n",
    "for j in tqdm(range(len(question_new))):\n",
    "    ques1=[]\n",
    "    for i in range(len(paragraph_new)):\n",
    "         ques1.append(cosineValue (question_new[j], paragraph_new[i]))\n",
    "    res = sorted(range(len(ques1)), key = lambda sub: ques1[sub])[-5:] \n",
    "    arr0=paragraph_new[res[0]]\n",
    "    arr1=paragraph_new[res[1]]\n",
    "    arr2=paragraph_new[res[2]]\n",
    "    arr3=paragraph_new[res[3]]\n",
    "    arr4=paragraph_new[res[4]]\n",
    "    para=paraoriginal_new[j]\n",
    "\n",
    "    check0=para==arr0\n",
    "    check1=para==arr1\n",
    "    check2=para==arr2\n",
    "    check3=para==arr3\n",
    "    check4=para==arr4\n",
    "\n",
    "    if(check2.all() or check3.all() or check4.all() ): # for top 3\n",
    "        count3+=1\n",
    "    if(check0.all() or check1.all() or check2.all() or check3.all() or check4.all() ): # for top 5\n",
    "        count5+=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "print((count3/len(question))*100)\n",
    "print((count5/len(question))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1238/1238 [00:03<00:00, 340.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# for each paragraph, breaking the paragraph into sentences and storing the vector of each sentence   (after removing stopwords)\n",
    "paravectors=[] # contains list of parasentence_new[i] which is containing list of sentence vectors of the particular paragraph\n",
    "for i in tqdm(range(len(paragra))):\n",
    "    text=paragra[i]\n",
    "    a_list1 = nltk.tokenize.sent_tokenize(text)  # breaking the paragraph to sentences\n",
    "    tokenized_sents3 = [word_tokenize(j) for j in a_list1]\n",
    "    parasent=[]            \n",
    "    for i in range(len(tokenized_sents3)):\n",
    "        filtered_sentence = [w for w in tokenized_sents3[i] if not w in stop_words] \n",
    "        filtered_sentence = [] \n",
    "        for w in tokenized_sents3[i]: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence.append(w)   \n",
    "        parasent.append(filtered_sentence)    # storing each sentence after removing stopwords\n",
    "    parasentence_new=[]                    \n",
    "    for sentence in parasent:\n",
    "        parasentence_new.append(sent_vectorizer(sentence, model)) # contains list of sentence vectors of the particular paragraph\n",
    "    paravectors.append(parasentence_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which finds the accuracy of the most similar sentence in a given paragraph with a given question\n",
    "def cosineValue1(v1,parasentence_new):\n",
    "    cossimilar=[]\n",
    "    for i in range(len(parasentence_new)):\n",
    "        cossimilar.append(cosineValue(v1,parasentence_new[i]))\n",
    "    res1 = sorted(range(len(cossimilar)), key = lambda sub: cossimilar[sub])[-1:]\n",
    "    x= cossimilar[res1[0]]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:12:54<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "65.0\n",
      "71.2\n"
     ]
    }
   ],
   "source": [
    "# function  to find the accuracy of getting the correct paragraph (after considering only the most similar sentence in it) out of the 3 and top 5 matched paragraphs (and removing stopwords)\n",
    "count3=0\n",
    "count5=0\n",
    "for j in tqdm(range(len(question_new))):\n",
    "    ques1=[]\n",
    "    for i in range(len(paragraph_new)):\n",
    "         ques1.append(cosineValue1 (question_new[j],paravectors[i]))  # the parameters are the given question vector and list containing sentence vectors of a given paragraph\n",
    "    res = sorted(range(len(ques1)), key = lambda sub: ques1[sub])[-5:] \n",
    "    arr0=paragraph_new[res[0]]\n",
    "    arr1=paragraph_new[res[1]]\n",
    "    arr2=paragraph_new[res[2]]\n",
    "    arr3=paragraph_new[res[3]]\n",
    "    arr4=paragraph_new[res[4]]\n",
    "    para_j=paraoriginal_new[j]\n",
    "\n",
    "    check0=para_j==arr0\n",
    "    check1=para_j==arr1\n",
    "    check2=para_j==arr2\n",
    "    check3=para_j==arr3\n",
    "    check4=para_j==arr4\n",
    "    \n",
    "    if(check2.all() or check3.all() or check4.all() ): # for top 3\n",
    "        count3+=1\n",
    "    if(check0.all() or check1.all() or check2.all() or check3.all() or check4.all() ): # for top 5\n",
    "        count5+=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "print((count3/len(question))*100)\n",
    "print((count5/len(question))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Question is: \n",
      "\n",
      "which holiday was the terrorist bombing planned for?\n",
      "\n",
      "\n",
      "Top 5 matched paragraphs are: \n",
      "\n",
      "1.  following garreth mallory's promotion to m, on a mission in mexico city unofficially ordered by a posthumous message from the previous m, 007 james bond kills three men plotting a terrorist bombing during the day of the dead and gives chase to marco sciarra, an assassin who survived the attack. in the ensuing struggle, bond steals his ring, which is emblazoned with a stylised octopus, and then kills sciarra by kicking him out of a helicopter. upon returning to london, bond is indefinitely suspended from field duty by m, who is in the midst of a power struggle with c, the head of the privately-backed joint intelligence service, consisting of the recently merged mi5 and mi6. c campaigns for britain to form alongside 8 other countries \"nine eyes \", a global surveillance and intelligence co-operation initiative between nine member states, and uses his influence to close down the '00' section, believing it to be outdated.\n",
      "\n",
      "2.  in the first world war, devonport was the headquarters of western approaches command until 1941 and sunderland flying boats were operated by the royal australian air force. it was an important embarkation point for us troops for d-day. the city was heavily bombed by the luftwaffe, in a series of 59 raids known as the plymouth blitz. although the dockyards were the principal targets, much of the city centre and over 3,700 houses were completely destroyed and more than 1,000 civilians lost their lives. this was largely due to plymouth's status as a major port charles church was hit by incendiary bombs and partially destroyed in 1941 during the blitz, but has not been demolished, as it is now an official permanent monument to the bombing of plymouth during world war ii.\n",
      "\n",
      "3.  oklahoma city is on the i-35 corridor and is one of the primary travel corridors into neighboring texas and mexico. located in the frontier country region of the state, the city's northeast section lies in an ecological region known as the cross timbers. the city was founded during the land run of 1889, and grew to a population of over 10,000 within hours of its founding. the city was the scene of the april 19, 1995 bombing of the alfred p. murrah federal building, in which 168 people died. it was the deadliest terror attack in the history of the united states until the attacks of september 11, 2001, and remains the deadliest act of domestic terrorism in u.s. history.\n",
      "\n",
      "4.   argentina: the torch relay leg in buenos aires, argentina, held on april 11, began with an artistic show at the lola mora amphitheatre in costanera sur. in the end of the show the mayor of buenos aires mauricio macri gave the torch to the first torchbearer, carlos espínola. the leg finished at the buenos aires riding club in the palermo district, the last torchbearer being gabriela sabatini. the 13.8 km route included landmarks like the obelisk and plaza de mayo. the day was marked by several pro-tibet protests, which included a giant banner reading \"free tibet\", and an alternative \"human rights torch\" that was lit by protesters and paraded along the route the flame was to take. most of these protests were peaceful in nature, and the torch was not impeded. chinese immigrants also turned out in support of the games, but only minor scuffles were reported between both groups. runners surrounded by rows of security carried the olympic flame past thousands of jubilant argentines in the most trouble-free torch relay in nearly a week. people showered the parade route with confetti as banks, government offices and businesses took an impromptu half-day holiday for the only latin american stop on the flame's five-continent journey.\n",
      "\n",
      "5.  \n",
      " japan: the event was held in nagano, which hosted the 1998 winter olympics, on april 26. japanese buddhist temple zenkō-ji, which was originally scheduled to be the starting point for the olympic torch relay in nagano, refused to host the torch and pulled out of the relay plans, amid speculation that monks there sympathized with anti-chinese government protesters. as well as the risk of disruption by violent protests. parts of zenkō-ji temple's main building (zenkō-ji hondō), reconstructed in 1707 and one of the national treasures of japan, was then vandalized with spraypaint. a new starting point, previously the site of a municipal building and now a parking lot, was chosen by the city. an event the city had planned to hold at the minami nagano sports park following the torch relay was also canceled out of concern about disruptions caused by demonstrators protesting against china's recent crackdown in tibet. thousands of riot police were mobilized to protect the torch along its route. the show of force kept most protesters in check, but slogans shouted by pro-china or pro-tibet demonstrators, japanese nationalists, and human rights organizations flooded the air. five men were arrested and four injured amidst scenes of mob violence. the torch route was packed with mostly peaceful demonstrators. the public was not allowed at the parking lot where the relay started. after the zenkoji monks held a prayer ceremony for victims of the recent events in tibet. more than 100 police officers ran with the torch and riot police lined the streets while three helicopters flew above. only two chinese guards were allowed to accompany the torch because of japan's concern over their treatment of demonstrators at previous relays. a man with a tibetan flag tried to stop the torch at the beginning of the relay but was dragged off by police. some raw eggs were also thrown from the crowd.\n",
      "\n",
      "\n",
      " Correct paragraph is :\n",
      "\n",
      "following garreth mallory's promotion to m, on a mission in mexico city unofficially ordered by a posthumous message from the previous m, 007 james bond kills three men plotting a terrorist bombing during the day of the dead and gives chase to marco sciarra, an assassin who survived the attack. in the ensuing struggle, bond steals his ring, which is emblazoned with a stylised octopus, and then kills sciarra by kicking him out of a helicopter. upon returning to london, bond is indefinitely suspended from field duty by m, who is in the midst of a power struggle with c, the head of the privately-backed joint intelligence service, consisting of the recently merged mi5 and mi6. c campaigns for britain to form alongside 8 other countries \"nine eyes \", a global surveillance and intelligence co-operation initiative between nine member states, and uses his influence to close down the '00' section, believing it to be outdated.\n"
     ]
    }
   ],
   "source": [
    "your_ques='which holiday was the terrorist bombing planned for?'\n",
    "\n",
    "train1 = train[train['question'].str.contains(your_ques)]\n",
    "train1.reset_index(inplace = True)\n",
    "x= train1.loc[0]['Unnamed: 0']     #  x: index of the given question in dataframe\n",
    "ques2=[]\n",
    "for i in range(len(paragraph)):     # iterate over paragraphs # no. of paragraph=1238\n",
    "  ques2.append(cosineValue1 (question_new[x], paravectors[i]))  # storing cosine value for a particular question with each paragraph\n",
    "res = sorted(range(len(ques2)), key = lambda sub: ques2[sub])[-5:]\n",
    "print(\"the Question is: \\n\")\n",
    "print(your_ques)\n",
    "print(\"\\n\\nTop 5 matched paragraphs are: \\n\")\n",
    "print(\"1. \",paragra[res[4]])\n",
    "print(\"\\n2. \",paragra[res[3]])\n",
    "print(\"\\n3. \",paragra[res[2]])\n",
    "print(\"\\n4. \",paragra[res[1]])\n",
    "print(\"\\n5. \",paragra[res[0]])\n",
    "print(\"\\n\\n Correct paragraph is :\\n\")\n",
    "print(paraorigi[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
